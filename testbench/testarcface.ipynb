{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff9a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.arcface import iresnet100\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577c9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = iresnet100()\n",
    "state_dict = torch.load(\"checkpoints/arcface-r100-glint360k.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8f4c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Amir\\Projects\\face\\src\\models\\arcface.py:192: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.fp16):\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 112, 112).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"checkpoints/arcface-r100-glint360k.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trtexec  --onnx=arcface-r100-glint360k.onnx --saveEngine=arcface-r100-glint360k_fp16.engine --fp16 --minShapes=input:1x3x112x112 --optShapes=input:2x3x112x112 --maxShapes=input:16x3x112x112\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b84bea",
   "metadata": {},
   "source": [
    "trtexec --onnx=arcface-r100-glint360k.onnx --saveEngine=arcface-r100-glint360k_fp16.engine --fp16 --workspace=2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "409189a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "def get_embedding(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_t = preprocess(img)\n",
    "    img_t = img_t.unsqueeze(0)\n",
    "    img_t = img_t.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(img_t)\n",
    "    emb = emb / emb.norm(p=2, dim=1, keepdim=True)\n",
    "    return emb.cpu().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f29cfbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PARATCO\\Projects\\face\\models\\arcface.py:149: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.fp16):\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding(\"img/amir.jpg\")\n",
    "embedding1 = get_embedding(\"img/download.png\")\n",
    "embedding2 = get_embedding(\"img/amir1.jpg\")\n",
    "sag = get_embedding(\"img/sag.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82d0c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinesim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01134903991243969"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinesim(embedding1, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a092f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import QDRANT_PORT, QDRANT_HOST\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import cv2 as cv\n",
    "\n",
    "collection_name = \"faces\"\n",
    "client = QdrantClient(QDRANT_HOST, grpc_port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd45d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    a = a.detach().numpy().ravel()\n",
    "    b = b.detach().numpy().ravel()\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa6a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(\"./testbench/img/faces/0.jpg\")\n",
    "cv.cvtColor(image, cv.COLOR_BGR2RGB, dst=image)\n",
    "image = np.transpose(image / 127.5 - 1.0, (2,0,1)).astype(np.float32)\n",
    "model.to('cpu')\n",
    "vec = model(torch.tensor([image]))\n",
    "# vec = vec[0].tolist()\n",
    "# client.query_points(collection_name, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cdc54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(\"./testbench/img/faces/amir.jpg\")\n",
    "cv.cvtColor(image, cv.COLOR_BGR2RGB, dst=image)\n",
    "image = np.transpose(image / 127.5 - 1.0, (2,0,1)).astype(np.float32)\n",
    "model.to('cpu')\n",
    "vec1 = model(torch.tensor([image]))\n",
    "# vec1 = vec1[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35386c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9687170386314392"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec, vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "468a082f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9687170386314392"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec, vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8803ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
