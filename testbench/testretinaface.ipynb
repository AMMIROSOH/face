{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff9a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.retinaface import cfg_re50\n",
    "from src.models.retinaface import retina50\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import cv2 as cv\n",
    "import time\n",
    "from src.utils.retinaface import PriorBox\n",
    "from torchvision.ops import nms\n",
    "from src.utils.arcface import estimate_norm, LANDS_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7271ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms\n",
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577c9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from checkpoints/RetinaFace-R50.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:456\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = retina50()\n",
    "model = load_model(model, \"checkpoints/RetinaFace-R50.pth\", device)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba07883",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"checkpoints/Resnet50_Final.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67836ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model forward time: 0.6724\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./testbench/img/testOmid.jpg\"\n",
    "img_raw = cv.imread(image_path, cv.IMREAD_COLOR)\n",
    "\n",
    "img = np.float32(img_raw)\n",
    "resize = 1\n",
    "\n",
    "scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "im_height, im_width, _ = img.shape\n",
    "img -= (104, 117, 123)\n",
    "img = img.transpose(2, 0, 1)\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(device)\n",
    "scale = scale.to(device)\n",
    "\n",
    "tic = time.time()\n",
    "loc, conf, landms = model(img)  # forward pass\n",
    "print('model forward time: {:.4f}'.format(time.time() - tic))\n",
    "\n",
    "priorbox = PriorBox(cfg_re50, image_size=(im_height, im_width))\n",
    "priors = priorbox.forward()\n",
    "priors = priors.to(device)\n",
    "prior_data = priors.data\n",
    "boxes = decode(loc.data.squeeze(0), prior_data, cfg_re50['variance'])\n",
    "boxes = boxes * scale / resize\n",
    "boxes = boxes.cpu().numpy()\n",
    "scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "landms = decode_landm(landms.squeeze(0), prior_data, cfg_re50['variance'])\n",
    "scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                        img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                        img.shape[3], img.shape[2]])\n",
    "scale1 = scale1.to(device)\n",
    "landms = landms * scale1 / resize\n",
    "landms = landms.cpu().detach().numpy()\n",
    "\n",
    "# ignore low scores\n",
    "inds = np.where(scores > 0.02)[0]\n",
    "boxes = boxes[inds]\n",
    "landms = landms[inds]\n",
    "scores = scores[inds]\n",
    "\n",
    "# keep top-K before NMS\n",
    "order = scores.argsort()[::-1][:5000]\n",
    "boxes = boxes[order]\n",
    "landms = landms[order]\n",
    "scores = scores[order]\n",
    "\n",
    "# do NMS\n",
    "dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "keep = py_cpu_nms(dets, 0.4)\n",
    "# keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n",
    "dets = dets[keep, :]\n",
    "landms = landms[keep]\n",
    "\n",
    "# keep top-K faster NMS\n",
    "dets = dets[:11, :]\n",
    "landms = landms[:11, :]\n",
    "\n",
    "dets = np.concatenate((dets, landms), axis=1)\n",
    "i = 0\n",
    "for d in dets:\n",
    "    if d[4] < 0.6:\n",
    "        continue\n",
    "    text = \"{:.4f}\".format(d[4])\n",
    "    d = list(map(int, d))\n",
    "\n",
    "    lm = landms[i]  # (10,)\n",
    "    landmarks = np.array([\n",
    "        [lm[0], lm[1]],   # left eye\n",
    "        [lm[2], lm[3]],   # right eye\n",
    "        [lm[4], lm[5]],   # nose\n",
    "        [lm[6], lm[7]],   # mouth left\n",
    "        [lm[8], lm[9]],   # mouth right\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    image = img_raw.copy()#cv.cvtColor(img_raw, cv.COLOR_BGR2RGB)\n",
    "    face_lands_norm = cv.estimateAffinePartial2D(landmarks, LANDS_TEMPLATE, method=cv.LMEDS)[0]\n",
    "    image = cv.warpAffine(img_raw, face_lands_norm, (112, 112), flags=cv.INTER_LINEAR)\n",
    "    # image = np.transpose(image / 127.5 - 1.0, (2,0,1)).astype(np.float32)\n",
    "    # cv.imshow(\"image\", image)\n",
    "    # cv.waitKey(5000)\n",
    "    # cv.destroyAllWindows()\n",
    "    name = f\"E:/Amir/Projects/face/testbench/img/faces/{i}.jpg\"\n",
    "    cv.imwrite(name, image)\n",
    "    i+=1\n",
    "\n",
    "    cv.rectangle(img_raw, (d[0], d[1]), (d[2], d[3]), (0, 0, 255), 2)\n",
    "    cx = d[0]\n",
    "    cy = d[1] + 12\n",
    "    cv.putText(img_raw, text, (cx, cy),\n",
    "                cv.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "    # landms\n",
    "    cv.circle(img_raw, (d[5], d[6]), 1, (0, 0, 255), 4)\n",
    "    cv.circle(img_raw, (d[7], d[8]), 1, (0, 255, 255), 4)\n",
    "    cv.circle(img_raw, (d[9], d[10]), 1, (255, 0, 255), 4)\n",
    "    cv.circle(img_raw, (d[11], d[12]), 1, (0, 255, 0), 4)\n",
    "    cv.circle(img_raw, (d[13], d[14]), 1, (255, 0, 0), 4)\n",
    "\n",
    "# save image\n",
    "\n",
    "# name = \"./testbench/test.jpg\"\n",
    "# cv.imwrite(name, img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e860f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
