{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.retinaface import cfg_re50\n",
    "from src.models.retinaface import retina50\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import cv2 as cv\n",
    "import time\n",
    "from src.utils.retinaface import PriorBox\n",
    "from torchvision.ops import nms\n",
    "from src.utils.arcface import estimate_norm, LANDS_TEMPLATE\n",
    "\n",
    "\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms\n",
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = retina50()\n",
    "model = load_model(model, \"checkpoints/RetinaFace-R50.pth\", device)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d111",
   "metadata": {},
   "source": [
    "# 1. extract wraped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68cac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./testbench/img/testOmid.jpg\"\n",
    "img_raw = cv.imread(image_path, cv.IMREAD_COLOR)\n",
    "\n",
    "img = np.float32(img_raw)\n",
    "resize = 1\n",
    "\n",
    "scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "im_height, im_width, _ = img.shape\n",
    "img -= (104, 117, 123)\n",
    "img = img.transpose(2, 0, 1)\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(device)\n",
    "scale = scale.to(device)\n",
    "\n",
    "tic = time.time()\n",
    "loc, conf, landms = model(img)  # forward pass\n",
    "print('model forward time: {:.4f}'.format(time.time() - tic))\n",
    "\n",
    "priorbox = PriorBox(cfg_re50, image_size=(im_height, im_width))\n",
    "priors = priorbox.forward()\n",
    "priors = priors.to(device)\n",
    "prior_data = priors.data\n",
    "boxes = decode(loc.data.squeeze(0), prior_data, cfg_re50['variance'])\n",
    "boxes = boxes * scale / resize\n",
    "boxes = boxes.cpu().numpy()\n",
    "scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "landms = decode_landm(landms.squeeze(0), prior_data, cfg_re50['variance'])\n",
    "scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                        img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                        img.shape[3], img.shape[2]])\n",
    "scale1 = scale1.to(device)\n",
    "landms = landms * scale1 / resize\n",
    "landms = landms.cpu().detach().numpy()\n",
    "\n",
    "# ignore low scores\n",
    "inds = np.where(scores > 0.02)[0]\n",
    "boxes = boxes[inds]\n",
    "landms = landms[inds]\n",
    "scores = scores[inds]\n",
    "\n",
    "# keep top-K before NMS\n",
    "order = scores.argsort()[::-1][:5000]\n",
    "boxes = boxes[order]\n",
    "landms = landms[order]\n",
    "scores = scores[order]\n",
    "\n",
    "# do NMS\n",
    "dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "keep = py_cpu_nms(dets, 0.4)\n",
    "# keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n",
    "dets = dets[keep, :]\n",
    "landms = landms[keep]\n",
    "\n",
    "# keep top-K faster NMS\n",
    "dets = dets[:11, :]\n",
    "landms = landms[:11, :]\n",
    "\n",
    "dets = np.concatenate((dets, landms), axis=1)\n",
    "i = 0\n",
    "for d in dets:\n",
    "    if d[4] < 0.6:\n",
    "        continue\n",
    "    text = \"{:.4f}\".format(d[4])\n",
    "    d = list(map(int, d))\n",
    "\n",
    "    lm = landms[i]  # (10,)\n",
    "    landmarks = np.array([\n",
    "        [lm[0], lm[1]],   # left eye\n",
    "        [lm[2], lm[3]],   # right eye\n",
    "        [lm[4], lm[5]],   # nose\n",
    "        [lm[6], lm[7]],   # mouth left\n",
    "        [lm[8], lm[9]],   # mouth right\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    image = img_raw.copy()#cv.cvtColor(img_raw, cv.COLOR_BGR2RGB)\n",
    "    face_lands_norm = cv.estimateAffinePartial2D(landmarks, LANDS_TEMPLATE, method=cv.LMEDS)[0]\n",
    "    image = cv.warpAffine(img_raw, face_lands_norm, (112, 112), flags=cv.INTER_LINEAR)\n",
    "    # image = np.transpose(image / 127.5 - 1.0, (2,0,1)).astype(np.float32)\n",
    "    # cv.imshow(\"image\", image)\n",
    "    # cv.waitKey(5000)\n",
    "    # cv.destroyAllWindows()\n",
    "    name = f\"E:/Amir/Projects/face/testbench/img/faces/{i}.jpg\"\n",
    "    cv.imwrite(name, image)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c526839",
   "metadata": {},
   "source": [
    "# 2. Insert vectors to qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import torchvision.transforms as transforms\n",
    "import pycuda.autoinit  # initializes CUDA driver\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import cv2 as cv\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# 1. Load the engine\n",
    "with open(\"checkpoints/arcface-r100-glint360k_fp16.engine\", \"rb\") as f:\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# 2. Create context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 3. Allocate buffers\n",
    "inputs, outputs, bindings, stream = [], [], [], cuda.Stream()\n",
    "\n",
    "for i in range(engine.num_io_tensors):\n",
    "    tensor_name = engine.get_tensor_name(i)\n",
    "\n",
    "    size = trt.volume(engine.get_tensor_shape(tensor_name))\n",
    "    dtype = trt.nptype(engine.get_tensor_dtype(tensor_name))\n",
    "\n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "\n",
    "    bindings.append(int(device_mem))\n",
    "    if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "        inputs.append((host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append((host_mem, device_mem))\n",
    "\n",
    "def infer(input_numpy):\n",
    "    context.set_input_shape(\"input\", input_numpy.shape)\n",
    "    # Copy input data to host buffer\n",
    "    np.copyto(inputs[0][0], input_numpy.ravel())\n",
    "\n",
    "    # Transfer to GPU\n",
    "    cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "\n",
    "    # Execute\n",
    "    context.execute_v2(bindings)\n",
    "\n",
    "    # Transfer outputs back\n",
    "    cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "    stream.synchronize()\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66644de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import QDRANT_PORT, QDRANT_HOST\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import os\n",
    "\n",
    "collection_name = \"faces\"\n",
    "client = QdrantClient(QDRANT_HOST, grpc_port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate( os.listdir(\"./testbench/img/faces\")):\n",
    "    name = filename.split(\".\")[0]\n",
    "    image = cv.imread(\"./testbench/img/faces/\"+ filename)\n",
    "    cv.cvtColor(image, cv.COLOR_BGR2RGB, dst=image)\n",
    "    image = np.transpose(cv.resize(image, (112,112)) / 127.5 - 1.0, (2,0,1)).astype(np.float32)\n",
    "    vec = infer(image)\n",
    "    vec = vec[0][0].tolist()\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name=collection_name, \n",
    "        points=[models.PointStruct(id=i+1, vector=vec, payload={\"name\": name})])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
