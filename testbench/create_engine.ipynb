{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f19b58",
   "metadata": {},
   "source": [
    "# Arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b09deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from models.arcface import iresnet100\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import onnx\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = iresnet100()\n",
    "state_dict = torch.load(\"../checkpoints/arcface-r100-glint360k.pth\", map_location=\"cpu\")\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Missing keys:\", missing_keys)\n",
    "print(\"Unexpected keys:\", unexpected_keys)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")     # ensure params on CPU\n",
    "dummy_input = torch.randn(1, 3, 112, 112)  # adjust to your model's input shape\n",
    "\n",
    "out_path = \"../checkpoints/arcface-r100-glint360k.onnx\"\n",
    "out_dir = Path(out_path).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        str(out_path),\n",
    "        export_params=True,          # make sure weights are embedded\n",
    "        opset_version=18,            # set >=18 as suggested by your log\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes=None,           # try None first; if you need dynamic axes use dynamic_shapes (below)\n",
    "        dynamo=False,                # force legacy exporter (disable torch.export/dynamo path)\n",
    "        verbose=False,\n",
    "        keep_initializers_as_inputs=True\n",
    "    )\n",
    "\n",
    "print(\"Export done ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a6e6e",
   "metadata": {},
   "source": [
    "### you should have Nvidia tensorrt installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec  --onnx=arcface-r100-glint360k.onnx --saveEngine=arcface-r100-glint360k_fp16.engine --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7029888",
   "metadata": {},
   "source": [
    "# Retinaface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc964c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.retinaface import retina50\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import onnx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms\n",
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = retina50()\n",
    "model = load_model(model, \"checkpoints/RetinaFace-R50.pth\", device)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")     # ensure params on CPU\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "out_path = \"../checkpoints/RetinaFace-R50.onnx\"\n",
    "out_dir = Path(out_path).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        str(out_path),\n",
    "        export_params=True,          # make sure weights are embedded\n",
    "        opset_version=18,            # set >=18 as suggested by your log\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes=None,           # try None first; if you need dynamic axes use dynamic_shapes (below)\n",
    "        dynamo=False,                # force legacy exporter (disable torch.export/dynamo path)\n",
    "        verbose=False,\n",
    "        keep_initializers_as_inputs=True\n",
    "    )\n",
    "\n",
    "print(\"Export done ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec  --onnx=RetinaFace-R50.onnx --saveEngine=RetinaFace-R50_fp16.engine --fp16"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
